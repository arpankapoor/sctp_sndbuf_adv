\chapter{Results}

\section{Use Case Description}
The test bed was used to create a scenario in which there are multiple
senders and multiple receivers. Multiple flows were created and the
bottleneck between the two Raspberry Pis were fully utilized. The aim
was to classify the flows according to the bandwidth requirements. The
TBF qdisc did not ensure fair sharing of bandwidth. The SFQ qdisc gives
very good performance but degrades for high number of flows \cite{lartc}. We
expect our method of classification can yield better results in reducing
average flow completion time in the above case.

\section{Test Results}
\begin{figure}[h]
  \centering
  \begin{gnuplot}[terminal=cairolatex]
    set xlabel 'Time (s)'
    set ylabel 'Percentage Send buffer Used\vspace{0.5cm}'
    set format y '%.0s'
    set key off
    set xrange [0:315]
    plot 'data/deep_tbf.dat' using ($1-5):2 lw 2 with lines,'data/prasad_tbf.dat' using ($1-5):2 lw 2 with lines
  \end{gnuplot}
  \caption{Percentage send buffer variation with 2 flows each having duration of
    300 seconds using the Token Bucket Filter qdisc with rate limited to
    1mbit/sec}
  \label{fig:tbf}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{gnuplot}[terminal=cairolatex]
    set xlabel 'Time (s)'
    set ylabel 'Percentage Send buffer Used\vspace{0.5cm}'
    set format y '%.0s'
    set key off
    set xrange [0:310]
    plot 'data/deep_sfq.dat' using ($1-80): 2 lw 2 with lines,'data/prasad_sfq.dat' using ($1-80): 2 lw 2 with lines
  \end{gnuplot}
  \caption{Percentage send buffer variation with 2 flows each having duration of
    300 seconds using Stochastic Fair Queuing qdisc with rate limited to
    1mbit/sec}
  \label{fig:sfq}
\end{figure}

\begin{table}[h]
  \centering
  \begin{tabular}{|c | c | c|}
    \hline
    Number of flows & Send buffer based hierarchy & SFQ \\ \hline
    20 & 2909.75 & 3651.0 \\
    200 & 296.53 & 365.65 \\
    1000 & 64.195 & 76.0317 \\
    2000 & 31.85 & 52.028 \\ \hline
  \end{tabular}
  \caption{Average bandwidth (kbits/sec) with different number of flows, each
    lasting 300 seconds}
  \label{table:1}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{|c | c | c|}
    \hline
    Number of flows & Send buffer based hierarchy & SFQ \\ \hline
    20 & 507.807 & 671.027 \\
    200 & 63.959 & 124.025 \\
    1000 & 29.193 & 107.742 \\
    2000 & 50.55 & 288.567 \\ \hline
  \end{tabular}
  \caption{Standard deviation in bandwidth (kbits/sec)}
  \label{table:2}
\end{table}

\clearpage

\section{Explanation of Test Results}
In figure \ref{fig:tbf}, one of the flows uses majority of the bandwidth for a
specific period of time during which the percentage send buffer occupancy of
the other host remains almost constant. On the whole, one of the end hosts
utilized four times the bandwidth utilized by the other end host.

In figure \ref{fig:sfq}, the bandwidth was shared equally between both the
flows. The percentage send buffer occupancy of both the hosts fluctuated
periodically during the test period.

The bandwidth observed using our implementation (table \ref{table:1}) was lesser than
the standard SFQ. This reduction can be mainly attributed to the overhead imposed by
the computation involved in classifying the packets based on send buffer
occupancy percentages. The additional overhead of 4 bytes per SCTP packet for
the send buffer chunk also affects the usable bandwidth.

As the number of flows increase, the probability of two flows hashing to the
same bucket also increases. This in turn increases the queuing delay for each of
the flows. In a standard SFQ implementation, there is no explicit indicator for
flows starved of bandwidth. Our implementation ensures that flows starved of
bandwidth are assigned a high priority leaf in the classification scheme.
Further each flow classified into a particular leaf is still fairly treated
owing to the SFQ qdisc in the leaf. This causes our implementation to have a
lesser variance in the bandwidth compared to the standard SFQ implementation.
